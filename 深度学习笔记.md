损失函数：**在单个训练样本中定义的 他衡量了 在单个训练样本上的表现**， 通常我们希望 损失函数的值越来越小 

​				 可以近似的理解为 方差！ y^ -> y的估计值! ^应该在y的上方，但我打不出来这个符号

​				深度学习中的 L(y^,y) = -( y log y^ + (1-y)log (1- y^) )

​				注： y^是由逻辑回归计算出来的，值域趋于（0，1）

​					y = 0 时  L(y^, y) = -log(1 - y^) 我们希望 输出结果变小，那么 y^就会趋于变小， y^-> 0

​					y = 1 时  L(y^, y) = -log(y^) 我们希望 输出结果变小， 那么y^就会趋于变大 y^ 就会趋于1

​	   其实有很多函数，都可以做到上述的效果 ，功能 呢 也类似于方差 ，但是 为了 后续的优化 ，我们才会选择 上		述的那个损失函数，原因呢，，，我才学到2.3我也不太清楚。

------

成本函数：衡量了在所有训练样本上的表现， 它是对 损失函数的所有结果 求平均值 ，我们希望成本函数的结果尽可能的小

------

梯度下降法 ： 假设是 ⊙ 实数， J是成本函数 ， 加入他们的关系是一个 二元函数， α右边的是 对于当前⊙的 导数，

![1578212507209](C:\Users\LJW20\AppData\Roaming\Typora\typora-user-images\1578212507209.png)

​					我们可以看到，如果将⊙定义在最优解的左边，那么 他就会沿着曲线像右走，。

​					如果在最优解的左边，就会沿着曲线向左走。直到走到最低点为止。

   				对于多维的函数也是一样，多维函数的话，就用偏导数的方式去进行计算

![img](https://upload-images.jianshu.io/upload_images/1234352-bb7fa36d116fcadc.png?imageMogr2/auto-orient/strip|imageView2/2/format/webp)

------

m个样本的 梯度下降：

​		 首先先针对一个样本，通过这个样本，我们可以计算出 成本函数对于 一个 参数的导数， 然后 就可以通过梯度下降法，计算出这个参数的最优解

​		针对多个样本时，我们遍历所有的样本集，然后计算每个样本集的 参数的导数，然后把它们累加起来，再除以样本集的个数，得到全局的导数，然后再进行梯度下降法，算出最优的 参数

------

python 广播：

​	![1578295548015](C:\Users\LJW20\AppData\Roaming\Typora\typora-user-images\1578295548015.png)  

------

多节点神经网络的向量表示：（ 要多熟悉向量表示！）

![1578475717222](C:\Users\LJW20\AppData\Roaming\Typora\typora-user-images\1578475717222.png)

------

训练误差 和 贝叶斯误差之间的差距，是可避免偏差 ， 开发集误差到训练误差之间是方差

缩小可避免偏差 ：

1. 更大的模型
2. 更好的神经网络架构 / 更好的超参数 / 改变激活函数。。
3. 训练更久/ 用更好的优化算法

缩小方差：

1. 使用更多数据训练
2. 正则化 L2 dropout
3. 也可以用不同的架构，超参数搜索

------

Data mismatch problem:

1. 



迁移学习： 利用相似领域学习的低级特征来加速训练! 

多任务学习：某些任务的低级特征是相同的，这样做可能会提高性能



端到端学习：

优点： 

1. 让数据说话（不拘泥于人的思维和成见，让算法自动发现数据的特征）
2. 减少手工设计中间组件的过程

缺点：

1. 需要较多数据。。
2. 有可能将有用的，精致的中间组件排斥在外。





首先建立一个基础模型先训练，然后再慢慢去向着你的目标优化









t-SNE算法，可以将高维度词向量压缩到二维可视化

tf.convert_to_tensor(a, dtype=tf.) 也是类型转换 但是a得类型只能是数字数组，python列表和python标量

tf.cast(a, dtype=tf.) 类型转换

拼接与分割

tf.concat ( )  矩阵拼接 除了拼接的维度以外其余必须相同

tf.stack() 矩阵堆叠 所有维度必须相同，用于生成新的维度

tf.unstack() 维度拆分，将某个维度拆分成两个

tf.split() 维度拆分，可以指定比例

数据统计

tf.norm(matrix a,[ord=, axis=])

ord = 1 一范数 ord=2 二范数

axis 指定求范数的轴，默认全求



reduce_min/max/mean() 求最大值， 最小值，均值  指定axis可以指定计算的维度

tf.argmax/argmin()  找到某维度上最值所在的索引 axis 默认为0   **返回的是int64位的数据**

tf.equal(a, b)  比较矩阵 a 和 b中的数据是否相等（float 和 int 不可比）， 返回布尔矩阵

tf.unique(c) 只接受一维数组， 返回去重复后的数组, 以及重复索引（tf.gather 可以用这个索引复原原数组）

向量排序

tf.sort(c, direction='DESCENDING') 将c排序

tf.argsort( 参数同上) 返回和c维度相同的数组，然后记录的是每个数的索引，并将该索引排序 （结合tf.gather就可以完成排序）

**但是上述两种算法只会对最低维度进行排序**

tf.math.top_k(a, k) 找出 a数组中前k个大的数据 ，以(indices，values)的形式返回



数据的填充与复制

tf.pad(matrix m, [[a, b], [c, d]])  对矩阵m进行填充， 后面传入一个二维矩阵，ab cd代表从两个维度进行填充， 有几个维度就弄几个【a, b】

tf.tile(matrix m, [a, b]) a 是第一个维度的复制次数 b是第二个维度的复制次数

tf.broadcast_to(matrix m, [广播成什么样子2， 3， 3])



张量限幅

tf.clip_by_value(matrix m, 2, 8)  把m中的所有数字，小于2的全弄成2 大于8的全弄成8 2~8的保原样

tf.nn.relu(a)   小于0的全弄成0， 大于0的保持不变

 tf.clip_by_norm(matrix m, a) 将矩阵m保持向量方向不变， 但是长度范数等比例缩小为15（这是一个新的norm）

还有一个tf.clip_by_global_norm(matrix, a) 计算所有 【w1, b1, w2, b2】等比例缩放



高阶操作

tf.where(matrix m) 将矩阵m中为True的元素 索引返回

tf.where(cond, A, B)  condition中为True的位置，从A矩阵中取， 为False的从B中取

tf.scantter_nd(indices, update, shape) shape 扩展的维度 ， update 是要更新的数据， indcies 是将 update的数据插入到更新后shape的索引

tf.linspace(-2., 2, 5) return-> [-2, -1, 0, 1, 2]

tf.meshgrid(x, y) x, y 由上一行生成，然后返回x， y组合后的points_x, points_y 维度均为 len(x) * len(y)



数据加载

tf.keras.datasets.mnist.load_data()

tf.keras.datasets.cifar10.load_data()



**tf.data.Dataset**

tf.data.Dataset.from_tensor_slices()

数据处理为可迭代对象， 可以仅返回 x ，也可以返回[x, y]的数组



.shuffle(1000) 把数据打散，避免神经网络学习到无用规律

.map(function fx)  就是map嘛

.batch(size)  就是制作成batch，然后还是一个可迭代对象 注意将y的 axis1维度squeeze掉，因为是1没啥用

.repeat( iternum)  迭代次数，不传入数据的话就是无限循环



### 神经网络开始！

##### 全连接层

net = tf.keras.layers.Dense(size， activation='规定激活函数', **kernel_regularizer=keras.regularizers.l2(0.001)**)  输出层的维度--》 规定一层神经元的数量

规定了正则化的方式

net.kernel ---> W  net.bias ---> b 

model = tf.keras.Sequential([layer1, layer2, layer3]) 将数据连接起来

连接好之后用 model.build(input_shape=[None, 输入维度])  就传入了

model.summary() 相当于print

model.trainable_variables===> 可训练参数

##### 输出方式

tf.sigmoid()

tf.softmax()

tf.tanh()

tf.nn.relu()

tf.nn.leaky_relu()

##### 误差计算

MSE   平方差的和 y 和 y_out

tf.reduce_mean(tf.square(y - out))

tf.reduce_mean(tf.losses.MSE(y, out))

Entropy 交叉熵

tf.losses.categorical_crossentropy(y_label, y_out, from_logits=False) 最后这个参数 不设置为True的话可能会导致数据不稳定的情况（具体怎么个不稳定，我还没见过） 但是默认为False



#### 随机梯度下降！

##### with tf.GradientTape(persistent=True) as tape: 用这个包裹起来！才可以跟踪计算过程，括号内参数表示计算结果可以多次使用（这个相当耗费资源。。。）

然后得有一个 tape.watch([w, b....])  如果变量是Variable类型的话不用写，如果是tensor类型的话，不写就没办法追踪计算过程

[dw, db.....] = tape.gradient(loss, [w, b....]) 然后就能自动求导！



#### Keras

datasets, layers, losses, optimizers,**metrics**

acc_meter = metrics.Accuracy()

loss_meter = metrics.Mean()



loss_meter.update_state(loss)

acc_meters.update_state(y, pred)



loss_meter.result().numpy()

acc_meter.result().numpy()



loss_meter.reset_states()

acc_meter.reset_states()

------

**Compile**

network.compile(optimizer=, loss=, metrics=)

**Fit**

network.fit(db, epochs=10, validation_data= 测试数据, validation_freq=循环几个db 测试一次数据)

network.fit(db_train_val, epochs=6, validation_split=0.1, validation_freq=2) 把数据全部扔进去，然后选一个 validation_split 这样就能自动给你划分所有数据的10%作为交叉验证集

**Evaluate**

network.evaluate(ds_val)

**Predict**

network.predict(x) 

------

**自定义层或网络**

keras.Sequential

keras.layers.Layer

keras.Model 

model.trainable_variables

model.call()

貌似和之前的没啥区别



**模型的加载与保存**

save/load weights(‘文件路径’)  保存模型的参数 / 读取模型的参数并且复制给当前的模型

save/load entire model('文件路径') 保存模型的全部参数信息简单粗暴。/ 读取模型的所有参数信息

saved_model(‘路径’) 直接生成可以让别的语言使用的工业化模型。



#### 动量梯度下降

optimizer = SGD(learning_rate=0.02, momentum=0.9)

optimizer = RMSprop(learning_rate=0.02, momentum=0.9)

optimizer = Adam(learning_rate=0.02, beta_1=0.9, beta_2=0.999)

optimizer.learning_rate = 0.2 * (100 - epoch)  类似这样的方法去让学习率衰减



#### Dropout

layers.Dropout(P) 直接用一个 Dropout层

前向传播的时候 需要设定network的（training=True）

test的时候，training=False

### NLP-----卷积神经网络！

#### Layer

layers.SimpleRNNCell(output_size)

layers.SimpleRNN(unit, dropout=0.5, return_sequences=True, unroll=True) 所有的中间层都要返回这个sequences， 只用于多层RNN网络